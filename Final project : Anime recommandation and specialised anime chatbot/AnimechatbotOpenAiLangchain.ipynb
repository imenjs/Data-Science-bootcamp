{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKtk5rAOELmo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MC8Le7HAr1H",
        "outputId": "dad3f62a-0dbe-4c33-bae6-3ca2893d7c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.11.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.34.0 watchdog-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIawD653sl2Z",
        "outputId": "873d2a12-c529-48cd-97cf-ee2ac2688fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "+ off@0.0.10\n",
            "added 23 packages from 23 contributors and audited 23 packages in 2.308s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqq streamlit --progress-bar off\n",
        "!npm install -qqq localtunnel --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_wVJmaquDes",
        "outputId": "f336c8cc-5916-456a-95cc-70e006e74f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4891, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4790, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4891, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4790, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 776, in _parseNoCache\n",
            "    def _parseNoCache(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "^C\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpj3lek88p\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [01:19<00:00, 55.3MB/s]\n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ3o3xyjuTVk",
        "outputId": "7311f171-a020-4c2c-aefc-e7be46733933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m194.6/232.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C7EOfAxufBa",
        "outputId": "275b16af-0b90-4d4e-cd3a-c489184f8b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing rag_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_app.py\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import streamlit as st\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# List of file paths for the five PDFs\n",
        "pdf_files = [\n",
        "    \"/content/anime_wikipedia_content.pdf\",\n",
        "    \"/content/myanimelist_content_report.pdf\",\n",
        "    \"/content/Death_Note_Vol_01_of_12_Eng.pdf\",\n",
        "\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store the pages from all PDFs\n",
        "all_pages = []\n",
        "\n",
        "# Iterate over each PDF file\n",
        "for file_path in pdf_files:\n",
        "    # Load the PDF file using PyPDFLoader\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    # Load and split the pages from the PDF\n",
        "    pages = loader.load_and_split()\n",
        "    # Extend the list of all pages with the pages from the current PDF\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "# Now, the all_pages list contains all the pages from the five PDFs\n",
        "\n",
        "# Split the text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(all_pages)\n",
        "\n",
        "# Now, the docs variable contains the text split into chunks\n",
        "\n",
        "\n",
        "\n",
        "# llm\n",
        "llm = LlamaCpp(model_path=\"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens=2000,\n",
        "               temperature=0.7,\n",
        "               top_p=0.7,\n",
        "               n_gpu_layers=-1,\n",
        "               n_ctx=2048)\n",
        "\n",
        "# splitting\n",
        "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                               #chunk_overlap=200)\n",
        "\n",
        "#docs = text_splitter.split_documents(all_pages)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# vector database\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key='chat_history',\n",
        "                                  return_messages=True,\n",
        "                                  output_key='answer',  # Ensure this is 'answer'\n",
        "                                  chat_history=[],\n",
        "                                  max_history_size=1000)\n",
        "\n",
        "# prompt\n",
        "\n",
        "# Define prompt template using combined_pdf_text as context\n",
        "\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are a polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) Provide a concise and to-the-point answer. Avoid lengthy explanations unless necessary.\n",
        "  (3) If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (4) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Model's Opinion or Feeling:\n",
        "[INST]\n",
        "If you're asking about my opinion or feeling, please note that I am not a human. I provide responses based on the information available to me and do not have personal opinions or feelings.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=False,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Welcome Food Lovers\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"What do you wanna eat today! #ConversationalRetrievalChain\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = chain(prompt)\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(answer[\"answer\"])\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqFoSBJUGDjW",
        "outputId": "443e683a-0040-48c5-de79-cd209d05b52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing anime_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile anime_app.py\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import streamlit as st\n",
        "\n",
        "# Load anime content from PDFs\n",
        "pdf_files = [\n",
        "    \"/content/anime_wikipedia_content.pdf\",\n",
        "    \"/content/myanimelist_content_report.pdf\",\n",
        "    \"/content/Death_Note_Vol_01_of_12_Eng.pdf\",\n",
        "]\n",
        "\n",
        "all_pages = []\n",
        "for file_path in pdf_files:\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load_and_split()\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(all_pages)\n",
        "\n",
        "# Initialize llm\n",
        "llm = LlamaCpp(model_path=\"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens=2000,\n",
        "               temperature=0.7,\n",
        "               top_p=0.7,\n",
        "               n_gpu_layers=-1,\n",
        "               n_ctx=2048)\n",
        "\n",
        "# Initialize embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n",
        "\n",
        "# Initialize vector database\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer',\n",
        "                                  chat_history=[], max_history_size=1000)\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are an anime expert chatbot. You must provide helpful responses to anime-related questions.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Start your answer with a friendly tone.\n",
        "  (1) Reiterate the question to confirm understanding.\n",
        "  (2) Provide a concise and to-the-point answer.\n",
        "  (3) If you can't find the answer, respond appropriately.\n",
        "\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize prompt\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Initialize chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                              memory=memory, return_source_documents=False,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Anime Chatbot\")\n",
        "st.image(\"https://coisasdojapao.com/wp-content/uploads/2017/06/musicasdeanimesJap%C3%A3o.jpg\", use_column_width=True)\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state.messages:\n",
        "    with st.container():\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"**User:** {message['content']}\")\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            st.markdown(f\"**Bot:** {message['content']}\")\n",
        "\n",
        "# User input\n",
        "if user_input := st.text_input(\"You:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.spinner(\"Bot is thinking...\"):\n",
        "        answer = chain(user_input)\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer[\"answer\"]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu0XGIxHNY7I",
        "outputId": "791aa155-0577-4577-db54-cc32dc936444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/320.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO_0ZKVBM9ny",
        "outputId": "21311616-479e-4ceb-ae16-e9dd4c5e2b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing animeAI_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile animeAI_app.py\n",
        "#Open Ai and Langchain\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import streamlit as st\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-hS10ucdJALR0AJPnJajKT3BlbkFJb1CtJbqN8pDkFDxArBQn'\n",
        "\n",
        "# Load anime content from PDFs\n",
        "pdf_files = [\n",
        "    \"/content/anime_wikipedia_content.pdf\",\n",
        "    \"/content/myanimelist_content_report.pdf\"\n",
        "    #\"/content/Death_Note_Vol_01_of_12_Eng.pdf\",\n",
        "]\n",
        "\n",
        "all_pages = []\n",
        "for file_path in pdf_files:\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load_and_split()\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(all_pages)\n",
        "\n",
        "# Initialize llm\n",
        "llm = LlamaCpp(model_path=\"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens=2000,\n",
        "               temperature=0.7,\n",
        "               top_p=0.7,\n",
        "               n_gpu_layers=-1,\n",
        "               n_ctx=2048)\n",
        "\n",
        "# Initialize embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n",
        "\n",
        "# Initialize vector database\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer',\n",
        "                                  chat_history=[], max_history_size=1000)\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are an anime expert chatbot. You must provide helpful responses to anime-related questions.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Start your answer with a friendly tone.\n",
        "  (1) Reiterate the question to confirm understanding.\n",
        "  (2) Provide a concise and to-the-point answer.\n",
        "  (3) If you can't find the answer, respond appropriately.\n",
        "\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize prompt\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Initialize chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                              memory=memory, return_source_documents=False,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"lets talk anime\")\n",
        "st.image(\"https://coisasdojapao.com/wp-content/uploads/2017/06/musicasdeanimesJap%C3%A3o.jpg\", use_column_width=True)\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state.messages:\n",
        "    with st.container():\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"**User:** {message['content']}\")\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            st.markdown(f\"**Bot:** {message['content']}\")\n",
        "\n",
        "# User input\n",
        "if user_input := st.text_input(\"You:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.spinner(\"Netro is thinking...\"):\n",
        "        answer = chain(user_input)\n",
        "        if answer[\"answer\"]:  # Check if Langchain provided a response\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer[\"answer\"]})\n",
        "        else:  # If Langchain didn't provide a response, use OpenAI\n",
        "            openai_prompt = f\"Question: {user_input}\\nContext: Anime Chatbot\\n\"\n",
        "            openai_response = openai.Completion.create(\n",
        "                engine=\"text-davinci-002\",  # Choose the engine based on your preferences\n",
        "                prompt=openai_prompt,\n",
        "                temperature=0.7,\n",
        "                max_tokens=100\n",
        "            ).choices[0].text.strip()\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": openai_response})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I-sXwH98vnP",
        "outputId": "3478c6fe-abc0-4ed8-98f0-5f5ac5a284d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.6)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.57)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioSgyuMe87lN",
        "outputId": "f9771823-a52a-4262-c315-29548df9b87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.6)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.57)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "g4rASrkO8rqP",
        "outputId": "97b26f76-a215-44b9-8a6d-965a262c2a0b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'LlamaGPT3' from 'langchain.llms' (/usr/local/lib/python3.10/dist-packages/langchain/llms/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-aa22d5ded1f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaGPT3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaGPT3' from 'langchain.llms' (/usr/local/lib/python3.10/dist-packages/langchain/llms/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain.llms import LlamaGPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NXZCHe3pKMf"
      },
      "outputs": [],
      "source": [
        "%%writefile animeAILLM_app.py\n",
        "#Open Ai and Langchain instead on mistral\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import streamlit as st\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-hS10ucdJALR0AJPnJajKT3BlbkFJb1CtJbqN8pDkFDxArBQn'\n",
        "\n",
        "# Load anime content from PDFs\n",
        "pdf_files = [\n",
        "    \"/content/anime_wikipedia_content.pdf\",\n",
        "    \"/content/myanimelist_content_report.pdf\"\n",
        "    #\"/content/Death_Note_Vol_01_of_12_Eng.pdf\",\n",
        "]\n",
        "\n",
        "all_pages = []\n",
        "for file_path in pdf_files:\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load_and_split()\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(all_pages)\n",
        "\n",
        "# Set up Langchain with OpenAI's GPT-3\n",
        "llm = LlamaGPT3(api_key='sk-proj-hS10ucdJALR0AJPnJajKT3BlbkFJb1CtJbqN8pDkFDxArBQn', engine='text-davinci-002')\n",
        "\n",
        "# Initialize embeddings, vector database, memory, and prompt as before\n",
        "\n",
        "# Initialize chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                              memory=memory, return_source_documents=False,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "\n",
        "# Initialize embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n",
        "\n",
        "# Initialize vector database\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer',\n",
        "                                  chat_history=[], max_history_size=1000)\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are an anime expert chatbot. You must provide helpful responses to anime-related questions.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Start your answer with a friendly tone.\n",
        "  (1) Reiterate the question to confirm understanding.\n",
        "  (2) Provide a concise and to-the-point answer.\n",
        "  (3) If you can't find the answer, respond appropriately.\n",
        "\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize prompt\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Initialize chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "                                              memory=memory, return_source_documents=False,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrn6jbj8MpkA",
        "outputId": "ceb4a77f-e8fd-4001-cfd8-c518547a6549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CtzNFDG7nQX",
        "outputId": "48c235ff-ba43-4917-9d77-a32d80d34afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ],
      "source": [
        "pip install openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIL2o27x8LZx"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = \"sk-proj-twv0jNl4WAZBOXCCoP1gT3BlbkFJX38wSiu4TXLbtetHisqp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR9rnpvw3UQt",
        "outputId": "7da3b880-6b0a-497c-e058-159ba2355bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting animeopenai_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile animeopenai_app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client with your API key\n",
        "client = OpenAI(api_key=\"sk-proj-twv0jNl4WAZBOXCCoP1gT3BlbkFJX38wSiu4TXLbtetHisqp\")\n",
        "\n",
        "st.markdown(\n",
        "    \"<h1 style='text-align: center; color: #FF5733;'> The Anime Alchemist</h1>\",\n",
        "    unsafe_allow_html=True\n",
        "      )\n",
        "# Set page title and image\n",
        "st.title(\"Let's Talk Anime !\")\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "    <style>\n",
        "        .center {{\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "            height: 100vh; /* Set the height of the container to full viewport height */\n",
        "            background-color: #f0f2f6; /* Background color of the container */\n",
        "        }}\n",
        "    </style>\n",
        "    <div class=\"center\">\n",
        "        <img src=\"/content/drive/MyDrive/animedataset/isaac-netero-png-hunter-x-hunter-netero-png.png\" style=\"width: 200px;\">\n",
        "    </div>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state.messages:\n",
        "    with st.container():\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"**You:** {message['content']}\")\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            st.markdown(f\"**Netro:** {message['content']}\")\n",
        "\n",
        "# User input\n",
        "if user_input := st.text_input(\"You:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.spinner(\"Netro is thinking...\"):\n",
        "        openai_prompt = f\"Question: {user_input}\\nContext: Anime Chatbot\\n\"\n",
        "        openai_response = client.completions.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",  # Specify the correct model ID\n",
        "            prompt=openai_prompt,\n",
        "            stream=False  # Set stream to False\n",
        "        ).choices[0].text.strip()\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": openai_response})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5t-EHbsi2wg"
      },
      "outputs": [],
      "source": [
        "%%writefile animeopenai_app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client with your API key\n",
        "client = OpenAI(api_key=\"sk-proj-twv0jNl4WAZBOXCCoP1gT3BlbkFJX38wSiu4TXLbtetHisqp\")\n",
        "\n",
        "# Set page title and image\n",
        "st.title(\"Let's Talk Anime !\")\n",
        "st.image(\"/content/drive/MyDrive/animedataset/isaac-netero-png-hunter-x-hunter-netero-png.png\")#, width=350)\n",
        "\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state.messages:\n",
        "    with st.container():\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"**You:** {message['content']}\")\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            st.markdown(f\"**Netero:** {message['content']}\", unsafe_allow_html=True)\n",
        "\n",
        "# User input\n",
        "user_input = st.text_input(\"You:\")\n",
        "if user_input:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "openai_response = None\n",
        "\n",
        "if user_input:\n",
        "    with st.spinner(\"Netero is thinking...\"):\n",
        "        openai_prompt = f\"Question: {user_input}\\nContext: Anime Chatbot\\n\"\n",
        "        openai_response = client.completions.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=openai_prompt,\n",
        "            max_tokens=200,\n",
        "            stream=False\n",
        "        ).choices[0].text.strip()\n",
        "\n",
        "# Display assistant's response immediately\n",
        "if openai_response:\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": openai_response})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Working\n",
        "%%writefile animeopenai_app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client with your API key\n",
        "client = OpenAI(api_key=\"sk-proj-twv0jNl4WAZBOXCCoP1gT3BlbkFJX38wSiu4TXLbtetHisqp\")\n",
        "\n",
        "# Set page title and image\n",
        "st.title(\"Let's Talk Anime !\")\n",
        "st.image(\"/content/drive/MyDrive/animedataset/isaac-netero-png-hunter-x-hunter-netero-png.png\")\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state.messages:\n",
        "    with st.container():\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"**You:** {message['content']}\")\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            st.markdown(f\"**Netero:** <span style='color:red'>{message['content']}</span>\", unsafe_allow_html=True)\n",
        "\n",
        "# User input\n",
        "user_input = st.text_input(\"You:\", key=\"user_input\")\n",
        "if user_input:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    openai_prompt = f\"Question: {user_input}\\nContext: Anime Chatbot\\n\"\n",
        "    openai_response = client.completions.create(\n",
        "        model=\"gpt-3.5-turbo-instruct\",\n",
        "        prompt=openai_prompt,\n",
        "        max_tokens=200,\n",
        "        stream=False\n",
        "    ).choices[0].text.strip()\n",
        "\n",
        "    # Display assistant's response immediately\n",
        "    if openai_response:\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": openai_response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkOTVP5xJEmv",
        "outputId": "6b788c42-a0a1-41b8-e4b1-6cc4b3fa77b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting animeopenai_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile animeopenai_app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client with your API key\n",
        "client = OpenAI(api_key=\"sk-proj-twv0jNl4WAZBOXCCoP1gT3BlbkFJX38wSiu4TXLbtetHisqp\")\n",
        "\n",
        "st.markdown(\n",
        "    \"<h1 style='text-align: center; color: #FF5733;'> The Anime Alchemist</h1>\",\n",
        "    unsafe_allow_html=True\n",
        "      )\n",
        "# Set page title\n",
        "st.title(\"Let's Talk Anime !\")\n",
        "\n",
        "# Create a layout with two columns\n",
        "col1, col2 = st.columns([4, 2])  # Adjust the ratio as needed\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# User input\n",
        "user_input = col1.text_input(\"You:\", key=\"user_input\")\n",
        "if user_input:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    openai_prompt = f\"Question: {user_input}\\nContext: Anime Chatbot\\n\"\n",
        "    openai_response = client.completions.create(\n",
        "        model=\"gpt-3.5-turbo-instruct\",\n",
        "        prompt=openai_prompt,\n",
        "        max_tokens=200,\n",
        "        stream=False\n",
        "    ).choices[0].text.strip()\n",
        "\n",
        "    # Display assistant's response immediately\n",
        "    if openai_response:\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": openai_response})\n",
        "\n",
        "# Display chat messages from history\n",
        "for message in st.session_state.messages:\n",
        "    if message[\"role\"] == \"user\":\n",
        "        col1.markdown(f\"**You:** {message['content']}\")\n",
        "    elif message[\"role\"] == \"assistant\":\n",
        "        col1.markdown(f\"**Netero:** <span style='color:red'>{message['content']}</span>\", unsafe_allow_html=True)\n",
        "\n",
        "# Display image in the second column\n",
        "col2.image(\"/content/drive/MyDrive/animedataset/isaac-netero-png-hunter-x-hunter-netero-png.png\", width=400)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cggwjv8wcoJ4",
        "outputId": "a451e351-3d26-48cb-b1fa-d5bfb09584d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting animeopenai_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clbjg3lPxIYm",
        "outputId": "61753fca-f0f6-4aba-c24b-a84176d614f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.148.193.166\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.91s\n",
            "your url is: https://calm-facts-juggle.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run animeopenai_app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}